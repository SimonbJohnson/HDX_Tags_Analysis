{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdx.utilities.easy_logging import setup_logging\n",
    "from hdx.hdx_configuration import Configuration\n",
    "from hdx.data.dataset import Dataset\n",
    "import pandas as pd\n",
    "import ast\n",
    "from scipy import spatial\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from hdx.data.resource import Resource\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re \n",
    "import gensim\n",
    "import os;\n",
    "import json;\n",
    "import re;\n",
    "import logging;\n",
    "import sqlite3;\n",
    "import sys;\n",
    "import multiprocessing;\n",
    "import matplotlib.pyplot as plt;\n",
    "from itertools import cycle;\n",
    "from io import StringIO\n",
    "import requests\n",
    "\n",
    "import Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling, Extracting, Collecting, and Preprocessing HDX datasets\n",
    "<ol>\n",
    "<li> Access HDX API and search for specific set of datasets.</li>\n",
    "<li> Create a dataframe from the json of datasets. </li>\n",
    "<li> Extract resource objects for each dataset (if the dataset has any). </li>\n",
    "<li> Extract the header (column names) of the datsets if it has url (public url). </li>\n",
    "<li> Save content to a file for further processing. </li>\n",
    "<li> Eliminate stop words </li> \n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add the 'geodata' to the dataset's metadata \n",
    "\n",
    "def add_geodata(df):\n",
    "    for i, row in df.iterrows():\n",
    "        if df.ix[i]['has_geodata'] == True:\n",
    "            df.at[i,'geodata'] = \"geodata\"\n",
    "    else:\n",
    "            df.at[i,'geodata'] = \" \"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add the country(ies) to the dataset's metadata\n",
    "\n",
    "def add_country(df):\n",
    "    for i, row in df.iterrows():\n",
    "    #print(row)\n",
    "        countryList=  df.ix[i]['solr_additions']\n",
    "        if type(countryList) is str:\n",
    "            countries = \"\"\n",
    "            res = re.findall(r'\\w+', countryList)\n",
    "            for j in range(1,len(res)):\n",
    "                countries += \" \" + res[j]\n",
    "            df.at[i,'country'] =  countries.lower()\n",
    "        else:\n",
    "            df.at[i,'country'] =  ''\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main fields of the metadata that will be used for the processing\n",
    "#'notes' is a description on the dataset.\n",
    "def project_dataframe(df):\n",
    "    columns = ['title','notes','tags','organization','dataset_source','geodata', 'country']\n",
    "    df = df[columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the header of the dataset (the columns)\n",
    "def extract_dataset_header(df):\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        try:\n",
    "            if df.ix[i]['id'] is not None and df.ix[i]['id'] != 0:\n",
    "                columns= extract_resource_header(df.ix[i]['id'])\n",
    "                df.at[i,'header'] = columns.lower()\n",
    "            else:\n",
    "                df.at[i,'header'] = \"\"\n",
    "        except:\n",
    "            #print(\"IO exception\")\n",
    "            continue\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the information of the organization that collect/share the dataset\n",
    "def extract_org_info(df):\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        tags = \" \"\n",
    "        organization = df.ix[i]['organization']\n",
    "        if type(organization) is str:\n",
    "            organization = ast.literal_eval(organization)\n",
    "        org = organization['description'] +\" \" + organization['title']\n",
    "        df.at[i,'organization'] = org.lower()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the content of all metadata fields into raw text\n",
    "def get_text(df):\n",
    "    #df.tags.astype(str)+ \" \" +\n",
    "    textdata =  df.title.fillna('').astype(str)+\" \" + df.header.fillna('').astype(str) +\" \" + df.organization.fillna('').astype(str) +\" \" +  df.notes.fillna('').astype(str)+\" \"+ df.country.fillna('').astype(str) + \" \" + df.geodata.fillna('').astype(str) \n",
    "    return textdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the content of all metadata fields into raw text\n",
    "def get_all_text(df):\n",
    "    #df.tags.astype(str)+ \" \" +\n",
    "    textdata =  df.title.fillna('').astype(str)+\" \" + df.header.fillna('').astype(str) +\" \" + df.organization.fillna('').astype(str) +\" \" +  df.notes.fillna('').astype(str)+\" \"+ df.country.fillna('').astype(str) + \" \" + df.geodata.fillna('').astype(str) \n",
    "    return textdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper func.\n",
    "from gensim.utils import simple_preprocess\n",
    "def read_datasets(row):\n",
    "    #print(row)\n",
    "    return simple_preprocess(str(row).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper func.\n",
    "def print_word_table(table, key):\n",
    "    return pd.DataFrame(table, columns=[key, 'similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the current tags of the dataset mainly for comparison and evaluation \n",
    "def extract_tags(df):\n",
    "    df['tag_list'] = [[] for _ in df.index]\n",
    "    for i, row in df.iterrows():\n",
    "        tags = \"\"\n",
    "        tag_list = []\n",
    "        tagvalues = df.ix[i]['tags']\n",
    "        if type(tagvalues) is str:\n",
    "            tagvalues = ast.literal_eval(tagvalues)\n",
    "        if type(tagvalues) is float:\n",
    "            tagvalues = []\n",
    "        for t in tagvalues:\n",
    "            #print( t['name'])\n",
    "            if t['name'] is not None:\n",
    "                tags+= t['name']+ \" \"\n",
    "                tag_list.append(t['name'])\n",
    "        if tags is None or tags.strip() ==\"\":\n",
    "            tags = \" \"\n",
    "        df.at[i,'tags'] = tags.lower()\n",
    "        df.at[i,'tag_list'] = tag_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def extract_resource_info(dataset):\n",
    " #   resource= dataset.get_resource()\n",
    "  #  return resource['url'], resource['created'].split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the resource objects for each dataset (if it has resource(s))\n",
    "def extract_resource(datasets):\n",
    "    dfObj = pd.DataFrame(columns=['id', 'package_id', 'url','date','format'])\n",
    "    for ds in datasets:\n",
    "        #print(ds)\n",
    "        if ds.is_requestable() == False:\n",
    "           #obj= None\n",
    "            obj = ds.get_resource(index=0)\n",
    "            #print(obj['format'])\n",
    "            dfObj = dfObj.append({'id': obj['id'], 'package_id':obj['package_id'],'url': obj['url'], 'date':obj['created'].split('-')[0],'format':obj['format'] },ignore_index=True)\n",
    "    return dfObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the xls and/or csv of using the url of the resources associated with the dataset\n",
    "def extract_resource_header(ds_id):\n",
    "    #resource = Resource.read_from_hdx(str(ds_id))\n",
    "    #if resource is not None:\n",
    "    #    header = \"\"\n",
    "\n",
    "    #    if resource['format'] == 'CSV':\n",
    "            #print(resource['url'])\n",
    "            #print(\"csv\" ,resource['url'])\n",
    "     #       s=requests.get(resource['url']).text\n",
    "     #       c=pd.read_csv(StringIO(s), nrows=2)#, header=None, error_bad_lines=False)\n",
    "     #       col_name = list(c.columns.values)\n",
    "     #       header = ' '.join(str(e) for e in col_name)\n",
    "     #   elif resource['format'] == 'XLS' or resource['format'] == 'XLSX':\n",
    "     #       c=pd.read_excel(resource['url'])#, sheetname=0, header=1)#StringIO(s)) #read_excel(BytesIO(s))#,header=None) #read_excel(StringIO(s))\n",
    "     #       col_name = list(c.columns.values)\n",
    "     #       header = ' '.join(str(e) for e in col_name)\n",
    "     #   else:\n",
    "     #       header = \"\"\n",
    "       # except IOError:\n",
    "         #   print(\"IO exception\")\n",
    "          #  continue\n",
    "            #pass\n",
    "            \n",
    "    #else:\n",
    "        #header = \"\"\n",
    "    header = \"\"\n",
    "    return header\n",
    "    #return resource['url'], resource['created'].split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://data.humdata.org/'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Connecting to HDX API\n",
    "Configuration.create(hdx_site='prod', user_agent='Crawling - Education datasets', hdx_read_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "extract_resource_header('cf0e8c85-e365-40f6-a7ab-58e98ad31e46')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_dataset=pd.read_csv(\"after_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Datasets\n",
      "creating data frame\n",
      "resource\n",
      "joining data\n",
      "filling gaps\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code snippets:\n",
    " 1) Access HDX API and request all the datasets that have tag = 'education'.\n",
    " 2) create a dataframe from the json of datasets\n",
    " 3) extract resource objects for each dataset (if the dataset has any).\n",
    " 4) use 'id', 'package_id' to join the resource object with the the dataset object.\n",
    " 5) extract the header (column names) of the datsets if it has url (public url)\n",
    " 6) save content to a file for further processing\n",
    "'''\n",
    "print(\"Getting Datasets\")\n",
    "datasets = Dataset.search_in_hdx(fq='', rows=100000)\n",
    "print('creating data frame')\n",
    "df = pd.DataFrame.from_dict(datasets, orient='columns')\n",
    "print('resource')\n",
    "df_resource= extract_resource(datasets)\n",
    "print('joining data')\n",
    "df.rename(columns={'id':'package_id'}, inplace=True)\n",
    "df = pd.merge(df, df_resource, on='package_id', how='outer')\n",
    "print('filling gaps')\n",
    "df['id'].fillna(0, inplace=True)\n",
    "df = extract_dataset_header(df)\n",
    "df.to_csv(\"metadat_before_cleaning_new.csv\", index=False)\n",
    "\n",
    "\n",
    "## Or load from prevoiusly prepared file \n",
    "#df = pd.read_csv(\"metadat_before_cleaning_new.csv\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code snippets:\n",
    " 1) add geodata\n",
    " 2) add country\n",
    " 3) extract org_info\n",
    " 4) extract tags\n",
    " 5) clean the content (number, special character,links and/or non-english chars removal )\n",
    " 6) concatenate the text content from all metdata fields.\n",
    "'''\n",
    "\n",
    "columns = ['id','header','title','tags','notes','total_res_downloads','organization','dataset_source','geodata', 'country']#,'location']\n",
    "Pcolumns = ['title','header','tags','notes','organization','dataset_source','geodata', 'country']#,'location']\n",
    "df = add_geodata(df)\n",
    "df = add_country(df)\n",
    "df_process = df[['title','notes','tags','header','organization','dataset_source','geodata', 'country']] #'tags'\n",
    "df_process = extract_org_info(df_process)\n",
    "df_process=  extract_tags(df_process)\n",
    "df_process = Utility.data_clean(df_process,Pcolumns)\n",
    "df_process['doc']  =  get_text(df_process)\n",
    "df_process['All_text'] = get_all_text(df_process)\n",
    "#text_data = df_process['doc'] #get_text(df_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c te ivoire food security indicators  food agriculture organization united nations food agriculture organization statistics collates disseminates food agricultural statistics globally division develops methodologies standards data collection holds regular meetings workshops support member countries develop statistical systems produce publications working papers statistical yearbooks that cover food security prices production trade agri environmental statistics u f te ivoire '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_process.ix[0]['doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                     [food security, hxl]\n",
       "1                                     [food security, hxl]\n",
       "2                                     [food security, hxl]\n",
       "3                                     [food security, hxl]\n",
       "4                                     [food security, hxl]\n",
       "                               ...                        \n",
       "10081    [airports, buildings, earthquakes, education, ...\n",
       "10082                   [cyclones - hurricanes - typhoons]\n",
       "10083    [cyclones - hurricanes - typhoons, tropical cy...\n",
       "10084    [cyclones - hurricanes - typhoons, geodata, ty...\n",
       "10085    [ebola, geodata, health, logistics, water sani...\n",
       "Name: tag_list, Length: 10086, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the tag_list of each dataset.\n",
    "df_process['tag_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words= set(stopwords.words('english'))\n",
    "stop_words.update(['unnamed','nan','file','xls','xlsx','zip','link', 'description','https'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save content after cleaning\n",
    "#df_process.to_csv(\"after_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process.to_csv(\"after_clean_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stop words removal.\n",
    "df_process['doc'] = Utility.remove_stopwords(df_process['doc'], stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_data =remove_stopwords(df_process['doc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_text_data =remove_stopwords(df_process['All_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_process['All_text'] =remove_stopwords(df_process['All_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save preprocessed content into file\n",
    "df_process.to_csv(\"after_preprocessing_new.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c te ivoire food security indicators  food agriculture organization united nations food agriculture organization statistics collates disseminates food agricultural statistics globally division develops methodologies standards data collection holds regular meetings workshops support member countries develop statistical systems produce publications working papers statistical yearbooks cover food security prices production trade agri environmental statistics u f te ivoire'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_process.ix[0]['doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
